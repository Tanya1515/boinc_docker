#!/usr/bin/env python

import argparse
import os
from functools import partial
import subprocess
from inspect import currentframe
from os.path import join, exists
import random
import tempfile
import json
import string
import xml.etree.cElementTree as ET
from xml.dom import minidom
from boinc2docker_create_new_app import add_new_app

def sh(cmd):
    return subprocess.check_output(cmd,shell=True,stderr=subprocess.STDOUT).strip()

# constructs a function that can format a given string s using both global variables
#and local variables from a specific frame f
fmt = partial(lambda s,f: s.format(**dict(globals(),**f.f_locals)),f=currentframe())

def create_directories(dir):
        if not exists(dir):
            os.makedirs(dir)

#create list of string for argument parsing function
def list_of_strings(arg):
        return arg.split(',')


def create_template_in(app_name, input_files_names):

        input_files_amount = len(input_files_names)

        #print("Creating input template for job...")

        root = ET.Element("input_template")
        for i in range(input_files_amount):
            fileinfo = ET.SubElement(root, "file_info")
            ET.SubElement(fileinfo, "number").text = str(i)
            

        workunit = ET.SubElement(root, "workunit")
        for i in range(input_files_amount):
            fileref = ET.SubElement(workunit, "file_ref")
            ET.SubElement(fileref, "file_number").text = str(i)
            ET.SubElement(fileref, "open_name").text = "boinc_docker" if (i == input_files_amount -1) else input_files_names[i].split("/")[-1]
            ET.SubElement(fileref, "copy_file")

        template_file = join(tmpdir(),'boinc2docker_in_'+ app_name)
        open(template_file,'w').write(minidom.parseString(ET.tostring(root, 'utf-8')).toprettyxml(indent=" "*4))

        return template_file

def create_template_out(app_name, output_files_names, workunit_number):

        #print("Creating output template for job...")

        with open("config.xml", "r+") as config:
                contents = config.readlines()
                for line in contents:
                        if "upload_url" in line:
                                upload_url = (line.split(">"))[1].split("<")[0]

        template_file = "/home/boincadm/project/templates/" + app_name + "_" + workunit_number + "_out"

        root = ET.Element("output_template")

        for i in output_files_names:
            fileinfo = ET.SubElement(root, "file_info")
            ET.SubElement(fileinfo, "name").text = i + "_" + workunit_number
            ET.SubElement(fileinfo, "generated_locally")
            ET.SubElement(fileinfo, "upload_when_present")
            ET.SubElement(fileinfo, "max_nbytes").text = "134217728"
            ET.SubElement(fileinfo, "url").text = upload_url
            
        result = ET.SubElement(root, "result")
        for i in output_files_names:
            fileref = ET.SubElement(result, "file_ref")
            ET.SubElement(fileref, "file_name").text = i + "_" + workunit_number
            ET.SubElement(fileref, "open_name").text = i
            ET.SubElement(fileref, "copy_file").text = "1"
            ET.SubElement(fileref, "optional").text = "1"

        open(template_file,'w').write(minidom.parseString(ET.tostring(root, 'utf-8')).toprettyxml(indent=" "*4))

        return "templates/" + app_name + "_" + workunit_number + "_out"


#get image id
def get_image_id(image):
        return sh('docker inspect --format "{{ .Id }}" '+image).strip().split(':')[1]

# tmp dir only created on-demand to reduce disk access
_tmpdir=[None]
def tmpdir():
        if _tmpdir[0] is None:
            _tmpdir[0] = tempfile.mkdtemp()
        return _tmpdir[0]

def get_manifest(image_path):
        return json.load(tarfile.open(image_path).extractfile('manifest.json'))

#save image and make
def create_image (app_name, image, input_files, docker_registry):

        path="/home/boincadm/project/docker_image"
        if not exists(path):
               os.mkdir(path)

        path = path + "/" + app_name
        if not exists(path):
               os.mkdir(path)

        for i in image:
            if (docker_registry != []):
                docker_registry_login = "docker login -u " + docker_registry[0] + " -p " + docker_registry[1] + " " + docker_registry[2]
                sh(docker_registry_login)

            try:
                image_id = get_image_id(i)
            except subprocess.CalledProcessError as e:
                if 'No such image' in e.output:
                    get_image = "docker pull " + i
                    sh(get_image)
                    image_id = get_image_id(i)
                else:
                    raise

            image_filename_tar = "image_" + image_id + ".tar"
            image_path = path + "/" + image_filename_tar

            if exists(image_path):
                #get description of layers and image
                manifest = get_manifest(image_path)
            else:
                #save docker image, than extracts all contents from tar archive
                #that is being piped in through stdin and then stores them in
                #the current directory specified by
                save_docker_image = "docker save " + i + "| tar xf - -C " + tmpdir()
                sh(save_docker_image)
                manifest = json.load(open(join(tmpdir(), 'manifest.json')))


            for layer in manifest[0]['Layers']:
                layer_id = layer.split("/")[0]
                layer_filename_tar = "layer_" + layer_id + ".tar"
                layer_path = path + "/" + layer_filename_tar

                #create tar file for layers of the image in verbose mode
                #print ("Creating tar layer archive %s"%layer_id[:12])
                layer_tar = "tar cvf " + layer_path + " -C " + tmpdir() + " " + layer_id
                sh(layer_tar)
                #print ("Creating gzip archive for layer %s"%layer_id[:12])
                layer_zip = "gzip -nfS .manual.gz " + layer_path
                sh(layer_zip)
                input_files.append(layer_path + ".manual.gz")


            #create tar file for image in verbose mode, it consists of image_id, manifest.json and repository
            #print ("Creating tar image archive %s"%image_id[:12])
            image_tar = "tar cvf " + image_path + " -C " + tmpdir() + " " + image_id + ".json " + "manifest.json repositories"
            sh(image_tar)
            #print ("Creating gzip archive for image %s"%image_id[:12])
            image_zip = "gzip -nfS .manual.gz " + image_path
            sh(image_zip)
            input_files.append(image_path + ".manual.gz")

        return 0


def get_image_size(image):
        return sh('docker inspect --format "{{ .Size }}" '+image)


def create_new_plan_class(plan_class_name, plan_class_args):

        #make new plan class to add to the configuration file
        plan_class_line = "     <plan_class>\n" + "        <name>" + plan_class_name + "</name>\n"

        plan_class_line = plan_class_line + "        <cuda/>\n" if plan_class_args['gpu_type'] == "nvidia" else plan_class_line + "        <cal/>" if plan_class_args['gpu_type'] == "amd" else plan_class_line
        if (plan_class_args['gpu_type'] != 'none'):
            plan_class_line = plan_class_line + "        <gpu_type>" + plan_class_args['gpu_type'] + "</gpu_type>\n"
            if (plan_class_args['ngpus'] != 0):
                plan_class_line = plan_class_line + "        <ngpus>" + plan_class_args['ngpus'] + "</ngpus>\n"
            if (plan_class_args['min_gpu_ram_mb'] != 0):
                plan_class_line = plan_class_line + "        <min_gpu_ram_mb>" + plan_class_args['min_gpu_ram_mb'] + "</min_gpu_ram_mb>\n"
            if (plan_class_args['gpu_ram_used_mb'] != 0):
                plan_class_line = plan_class_line + "        <gpu_ram_used_mb>" + plan_class_args['gpu_ram_used_mb'] + "</gpu_ram_used_mb>\n"
            if (plan_class_args['gpu_type'] == "amd" and plan_class_args['use_ati_libs']):
                plan_class_line = plan_class_line + "        <need_ati_libs/>\n"
            if (plan_class_args['gpu_type'] == "amd" and plan_class_args['use_amd_libs']):
                plan_class_line = plan_class_line + "        <need_amd_libs/>\n"
            if (plan_class_args['driver_versions'][0] != '0'):
                plan_class_line = plan_class_line + "        <min_driver_version>" + plan_class_args['driver_versions'][0] + "</min_driver_version>\n"
            if (plan_class_args['driver_versions'][1] != '0'):
                plan_class_line =  plan_class_line + "        <max_driver_version>" + plan_class_args['driver_versions'][1] + "</max_driver_version>\n"
            if (plan_class_args['gpu_type'] == "nvidia" and plan_class_args['cuda_versions'][0] != '0'):
                plan_class_line =  plan_class_line + "        <min_cuda_version>" + plan_class_args['cuda_versions'][0] + "</min_cuda_version>\n"
            if (plan_class_args['gpu_type'] == "nvidia" and plan_class_args['cuda_versions'][1] != '0'):
                plan_class_line =  plan_class_line + "        <max_cuda_version>" + plan_class_args['cuda_versions'][1] + "</max_cuda_version>\n"

        if (plan_class_args['min_ncpus'] > 0 ):
            plan_class_line = plan_class_line + "        <min_ncpus>" + plan_class_args['min_ncpus'] + "</min_ncpus>\n"
        if (plan_class_args['max_threads'] > 0):
            plan_class_line = plan_class_line + "        <max_threads>" + plan_class_args['max_threads'] + "</max_threads>\n"
        if (plan_class_args['mem_usage_base_mb'] > 0):
            plan_class_line = plan_class_line + "        <mem_usage_base_mb>" + plan_class_args['mem_usage_base_mb'] + "</mem_usage_base_mb>\n"
        if (plan_class_args['mem_usage_per_cpu_mb'] > 0):
            plan_class_line = plan_class_line + "        <mem_usage_per_cpu_mb>" + plan_class_args['mem_usage_per_cpu_mb'] + "</mem_usage_per_cpu_mb>\n"
        #if (plan_class_args['use_docker']):

        plan_class_line = plan_class_line + "        <docker/>\n"
        if ((plan_class_args['use_compose']) and (plan_class_args['use_docker_compose'])):
            plan_class_line = plan_class_line + "        <docker_compose_version>v1v2</docker_compose_version>\n"
        elif (plan_class_args['use_compose']):
            plan_class_line = plan_class_line + "        <docker_compose_version>v2</docker_compose_version>\n"
        elif (plan_class_args['use_docker_compose']):
            plan_class_line = plan_class_line + "        <docker_compose_version>v1</docker_compose_version>\n"

        plan_class_line = plan_class_line + "     </plan_class>\n"

        with open("plan_class_spec.xml", "r+") as plan_class_config:
                    contents = plan_class_config.readlines()
                    len_contents = len(contents)
                    for i in range(len_contents):
                            if i == (len_contents - 1):
                                    contents.insert(i, plan_class_line)

        with open("plan_class_spec.xml", "r") as file:
                    file.close()

        with open("plan_class_spec.xml", "w") as project_config:
                    project_config.writelines(contents)

        return 0


def create_input_files(input_files):
        for i in input_files:
            stage_input_files = "/home/boincadm/project/bin/stage_file " + i
            sh(stage_input_files)

def create_workunit_number():
        letters = string.ascii_lowercase + string.digits
        result_str = ''.join(random.choice(letters) for i in range(10))
        return result_str

def create_docker_script(image, input_files, app_name, workunit_number, bash_script):

        docker_script_path = "/home/boincadm/project/docker_image/" + app_name + "/boinc_docker_" + workunit_number

        if (image[0] != 'none'):
            line = "\nfor f in ./*.tar.manual.gz; do [ -e $f ] && gunzip -c $f > ./$(basename $f .manual.gz); done\n" + "\ncat $(for f in ./*.tar; do [ -e $f ] && echo $f; done) | tar xi -C ./ \n" + "\nrm  ./*.tar \n" + "\ntar cf - -C ./ . | docker load \n"
            with open(bash_script, "r+") as bash_script_file:
                script = bash_script_file.readlines()
                script.insert(1, line)

            with open(docker_script_path, "w") as docker_script:
                docker_script.writelines(script)
                
        input_files.append(docker_script_path)

        return 0


def create_new_job(app_name, image, new_app, plan_class_name, plan_class_new, plan_class_args, input_files, docker_registry, ngpus, output_files_names, bash_script):
        if plan_class_new and new_app:
            create_new_plan_class(plan_class_name, plan_class_args)

        if (image[0] != 'none'):
            create_image(app_name, image, input_files, docker_registry)

        if new_app:
            add_new_app(app_name, plan_class_name, input_files, ngpus, output_files_names)

        workunit_number = create_workunit_number()

        create_docker_script(image, input_files, app_name, workunit_number, bash_script)

        create_input_files(input_files)

        input_files_names = ""
        last_item = input_files[-1]
        for i in input_files:
            if (i != last_item):
                input_files_names = input_files_names + i.split("/")[-1]  + " "
            else:
                input_files_names = input_files_names + i.split("/")[-1]

        template_file_in = create_template_in(app_name, input_files)

        template_file_out = create_template_out(app_name, output_files_names, workunit_number)

        create_work_command = "/home/boincadm/project/bin/create_work --appname " + app_name + " --wu_template " +  template_file_in + " --result_template " + template_file_out + " --wu_name " + app_name + "_" + workunit_number + " " + input_files_names

        #print(create_work_command)

        sh(create_work_command)

        print('Your workunit ID is: %s' %workunit_number)


if __name__=='__main__':
    parser = argparse.ArgumentParser(prog='create_new_job')
    parser.add_argument('--appname', default='boinc_docker', help='appname (default: boinc2docker)')
    parser.add_argument('--new_app', action='store_true', help='add new application to the BOINC server')
    parser.add_argument('--image', type=list_of_strings, default=['none'], help='docker images to run (default: none), if your own docker registry is used, please add correct path to image')
    parser.add_argument('--use_docker_compose', action='store_true', help='require docker-compose (old version) use')
    parser.add_argument('--use_compose', action='store_true', help='require docker compose (new version) use')
    parser.add_argument('--plan_class_new', action='store_true', help='add new plan class to the BOINC server, if mentioned with flag new_app, than the application will be created with new specified plan class')
    parser.add_argument('--plan_class_name', default='docker_test', help='name of the plan class')
    parser.add_argument('--gpu_type', default='none', choices=['nvidia', 'amd'], help='type of the gpu')
    parser.add_argument('--min_gpu_ram_mb', default=0, help='minimum amount of GPU RAM in MB')
    parser.add_argument('--gpu_ram_used_mb', default=0, help='requirement of this much available GPU RAM in MB')
    parser.add_argument('--ngpus', default=0, help='amount of gpus to use')
    parser.add_argument('--driver_versions', type=list_of_strings, default=['0', '0'], help='range of available gpu driver versions' )
    parser.add_argument('--cuda_versions', type=list_of_strings, default=['0', '0'], help='range of available cuda versions, the flag is used only for nvidia gpus' )
    parser.add_argument('--use_ati_libs', action='store_true', help='require libraries named ati, the flag is used for ATI/AMD gpus')
    parser.add_argument('--use_amd_libs', action='store_true', help='require libraries named amd, the flag is used for ATI/AMD gpus')
    #parser.add_argument('--use_docker', action='store_true', help='require docker use')
    parser.add_argument('--min_ncpus', default=0, help='minimal amount of processors to use')
    parser.add_argument('--max_threads', default=0, help='maximal amount of threads to use')
    parser.add_argument('--mem_usage_base_mb', default=0, help='memmory usage')
    parser.add_argument('--mem_usage_per_cpu_mb', default=0, help='if specified, estimated memory usage is X + NY, where X is mem_usage_base_mb, N is amount of processors, Y is mem_usage_per_cpu_mb')
    parser.add_argument('--input_files', type=list_of_strings, default=[], help='list of input_files for the job')
    parser.add_argument('--docker_registry', type=list_of_strings, default=[], help='list of docker login options for using your own docker registry: user, password, docker registry name')
    parser.add_argument('--output_files_names', type=list_of_strings, default=['test'], help='list of output files names')
    parser.add_argument('--bash_script_path', default='none', help='path to bash script file of the user')
    args = parser.parse_args()

    plan_class_args = {'gpu_type': args.gpu_type, 'min_gpu_ram_mb':args.min_gpu_ram_mb, 'gpu_ram_used_mb':args.gpu_ram_used_mb, 'ngpus':args.ngpus, 'driver_versions':args.driver_versions, 'cuda_versions':args.cuda_versions, 'use_ati_libs':args.use_ati_libs, 'use_amd_libs': args.use_amd_libs, 'min_ncpus':args.min_ncpus, 'max_threads':args.max_threads, 'mem_usage_base_mb':args.mem_usage_base_mb, 'mem_usage_per_cpu_mb':args.mem_usage_per_cpu_mb, 'use_compose':args.use_compose, 'use_docker_compose':args.use_docker_compose }
    create_new_job(app_name=args.appname, image=args.image, new_app=args.new_app, plan_class_name=args.plan_class_name, plan_class_new=args.plan_class_new, plan_class_args=plan_class_args, input_files=args.input_files, docker_registry=args.docker_registry, ngpus=args.ngpus, output_files_names=args.output_files_names, bash_script=args.bash_script_path)

